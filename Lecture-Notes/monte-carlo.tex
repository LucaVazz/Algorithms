\chapter[Monte-Carlo Method]{Die Monte-Carlo-Methode}
Bestimmte Probleme sind so komplex, dass es mit vertretbarem Aufwand nicht m\"oglich ist, eine exakte L\"osung zu
berechnen.  Oft l\"asst sich jedoch mit Hilfe einer Simulation das Problem zumindest n\"aherungsweise l\"osen.  
\begin{enumerate}
\item Das Problem der Berechnung der Volumina von K\"orpern, die eine gro{\ss}e Zahl von Begrenzungsfl\"achen haben,
      l\"asst sich auf die Berechnung mehrdimensionaler Integrale zur\"uckf\"uhren.  In der Regel k\"onnen diese
      Integrationen aber nicht analytisch ausgef\"uhrt werden.   Mit der Monte-Carlo-Methode l\"asst sich hier
      zumindest ein N\"aherungswert bestimmen.
\item Die Gesetzm\"a{\ss}igkeiten des Verhaltens komplexer Systeme, die zuf\"alligen Ein\-fl\"ussen einer Umgebung
      ausgesetzt sind, k\"onnen oft nur durch Simulationen bestimmt werden.  Wird beispielsweise ein neues
      U-Bahn-System geplant, so wird die Kapazit\"at eines projektierten Systems durch Simulationen ermittelt.
\item Bei Gl\"uckspielen ist die exakte Berechnung bestimmter Wahrscheinlichkeiten oft nicht m\"oglich.
      Mit Hilfe von Simulationen lassen sich aber gute N\"aherungs\-werte bestimmen.  
\end{enumerate}
Die obige Liste k\"onnte leicht fortgesetzt werden.  In diesem Kapitel werden wir zwei Beispiele betrachten.
\begin{enumerate}
\item Als einf\"uhrendes Beispiel zeigen wir, wie sich mit Hilfe der Monte-Carlo-Methode Fl\"acheninhalte bestimmen
      lassen.  Konkret berechnen wir den Fl\"a\-cheninhalt eines Kreises und bestimmen auf diese Weise
      die Zahl $\pi$.
\item Als zweites Beispiel zeigen wir, wie sich Karten zuf\"allig mischen lassen.
      Damit kann beispielsweise die Wahrscheinlichkeit daf\"ur berechnet werden, dass im Texas Hold'em Poker eine
      gegebene Hand gegen eine zuf\"allige Hand gewinnt.
\end{enumerate}

\section[Computing $\pi$]{Berechnung der Kreiszahl $\pi$}
Eine sehr einfache Methode zur Berechnung einer Approximation der Zahl $\pi$ funktioniert wie folgt.
Wir betrachten in der reellen Ebene den Einheits-Kreis $E$, der als die Menge
\[ E = \{ \pair(x,y) \in \mathbb{R}^2 \mid x^2 + y^2 \leq 1 \} \]
definiert ist.  Der Ausdruck $\sqrt{x^2 + y^2}$ gibt nach dem Satz des Pythagoras gerade den Abstand an,
den der Punkt $\pair(x,y)$ vom Koordinatenursprung $\pair(0,0)$ hat.  Der Einheits-Kreis hat offenbar den
Radius $r = 1$.  Damit gilt f\"ur die Fl\"ache dieses Kreises
\[ \textsl{Fl\"ache}(E) = \pi \cdot r^2 = \pi. \]
Wenn es uns gelingt, diese Fl\"ache zu berechnen, dann haben wir also $\pi$ bestimmt.  Eine experimentelle
Methode zur Bestimmung dieser Fl\"ache besteht darin, dass wir in das Quadrat $Q$, dass durch
\[ Q = \{ \pair(x,y) \in \mathbb{R} \mid -1 \leq x \leq 1 \;\wedge\; -1 \leq y \leq 1 \} \]
definiert ist, zuf\"allig eine gro{\ss}e Zahl $n$ von Sandk\"ornern werfen.  Wir notieren uns dabei die Zahl $k$ 
der Sandk\"orner, die in den Einheits-Kreis fallen.  Die Wahrscheinlichkeit $p$ daf\"ur, dass ein Sandkorn in den
Einheits-Kreis f\"allt, wird nun proportional zur Fl\"ache des Einheits-Kreises sein:
\[ p = \frac{\textsl{Fl\"ache}(E)}{\textsl{Fl\"ache}(Q)}. \]
Da das Quadrat die Seitenl\"ange 2 hat, gilt f\"ur die Fl\"ache des Quadrats $Q$ die Formel
\[ \textsl{Fl\"ache}(Q) = 2^2 = 4. \]
Auf der anderen Seite wird bei einer hohen Anzahl von Sandk\"ornern das Verh\"altnis $\ds\frac{k}{n}$ gegen diese
Wahrscheinlichkeit $p$ streben, so dass wir insgesamt
\[ \frac{k}{n} \approx \frac{\pi}{4} \]
haben, woraus sich f\"ur $\pi$ die N\"aherungsformel
\[ \pi \approx 4 \cdot \frac{k}{n} \]
ergibt.  Anstatt mit Sand  k\"onnen wir dieses Experiment einfacher mit Hilfe eines Computers durchf\"uhren.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    approximatePi := procedure(n) {
        k := 0;  
        i := 0;
        while (i < n) {
            x := 2 * random() - 1;
            y := 2 * random() - 1;
            r := x * x + y * y;
            if (r <= 1) {
                k += 1;
            }
            i += 1;
        }
        return 4.0 * k / n;
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Experimentelle Bestimmung von $\pi$ mit Hilfe der Monte-Carlo-Methode.}
\label{fig:monte-carlo.stlx}
\end{figure}

Abbildung \ref{fig:monte-carlo.stlx} zeigt die Funktion \texttt{approximatePi}, die mit dem oben
beschriebenen Verfahren einen N\"aherungswert f\"ur $\pi$ berechnet.
\begin{enumerate}
\item Der Parameter $n$ gibt die Anzahl der Sandk\"orner an, die wir in das Quadrat $Q$ werfen.
\item Um ein Sandkorn zuf\"allig zu werfen, werden mit Hilfe der Funktion $\textsl{random}()$ 
      zun\"achst Zufalls\-zahlen erzeugt, die in dem
      Intervall $[0,1]$ liegen.  Mit Hilfe der Transformation 
      \[ t \mapsto 2 \cdot t - 1  \]
      wird das Intervall $[0,1]$ in das Intervall $[-1, 1]$ transformiert, so dass die in den Zeilen 5 und 6
      berechneten Koordinaten \texttt{x} und \texttt{y} ein zuf\"allig in das Quadrat $Q$ 
      geworfenes Sandkorn beschreiben.
\item Wir berechnen in Zeile 7 das Quadrat des  Abstandes dieses Sandkorns vom
      Koordinatenursprung und \"uberpr\"ufen in Zeile 8, ob das Sandkorn innerhalb des Kreises liegt.
\end{enumerate}

\begin{table}[htbp]
  \label{tab:pi}
  \centering
  \begin{tabular}[t]{|r|r|r|}
  \hline
  n & N\"aherung f\"ur $\pi$ & Fehler der N\"aherung \\
  \hline
  \hline
               $10$ & 2.40000 & -0.741593 \\
\hline
              $100$ & 3.28000 & +0.138407 \\
\hline
           $1\,000$ & 3.21600 & +0.074407 \\
\hline
          $10\,000$ & 3.13080 & -0.010793 \\
\hline
         $100\,000$ & 3.13832 & -0.003273 \\
\hline
      $1\,000\,000$ & 3.13933 & -0.002261 \\
\hline
     $10\,000\,000$ & 3.14095 & -0.000645 \\
\hline
    $100\,000\,000$ & 3.14155 & -0.000042 \\
\hline
 $1\,000\,000\,000$ & 3.14160 & +0.000011 \\
\hline
  \end{tabular}
  \caption{Ergebnisse bei der Bestimmung von $\pi$ mit der Monte-Carlo-Methode}
\end{table}

Lassen wir das Progamm laufen, so erhalten wir die in Tabelle \ref{tab:pi} gezeigten Ergebnisse.
Wir sehen, dass wir zur Berechnung von $\pi$ mit einer Genauigkeit von $10^-{3}$ etwa $10\,000\,000$
Versuche brauchen, was angesichts der Rechenleistung heutiger Computer kein Problem darstellt.  Die Berechnung
weiterer Stellen gestaltet sich jedoch sehr aufwendig:  Eine Genauigkeit von $10^{-4}$
erfordert schon $1\,000\,000\,000$ Versuche. Grob gesch\"atzt k\"onnen wir
sagen, dass sich der Aufwand bei der Berechnung jeder weiteren Stelle verhundertfacht!  
Wir halten folgende Beobachtung fest:

\begin{center}
\colorbox{red}{\colorbox{orange}{\framebox{
  \framebox{
  \begin{minipage}[t]{12cm}
  Die Monte-Carlo-Methode ist gut geeignet, um grobe Absch\"atzungen zu berechnen, wird aber sehr aufwendig,
  wenn eine hohe Genauigkeit gefordert ist.
  \end{minipage}
  }}}}
\end{center}

\section[Theory]{Theoretischer Hintergrund$^*$}
Wir diskutieren nun den theoretischen Hintergrund der Monte-Carlo-Methode.  Da im zweiten Semester noch
keine detailierteren Kenntnisse aus der Wahrscheinlichkeits\-rechnung vorhanden sind, beschr\"anken wir
uns darauf, die wesentlichen Ergebnisse anzugeben.  Eine Begr\"undung dieser Ergebnisse erfolgt dann
in der Statistik-Vorlesung im vierten Semester. 

Bei der Monte-Carlo-Methode wird ein Zufalls-Experiment, im gerade diskutierten Beispiel war es das Werfen
eines Sandkorns, sehr oft wiederholt.  F\"ur den Ausgang dieses Zufalls-Experiments gibt es dabei zwei
M\"oglichkeiten:  Es ist ent\-weder erfolgreich (im obigen Beispiel landet das Sandkorn im Kreis) oder nicht
erfolgreich.  Ein solches Experiment bezeichnen wir als \emph{Bernoulli-Experiment}.
 Hat die Wahrscheinlichkeit, dass das Experiment erfolgreich ist, den Wert $p$ und wird das
Experiment $n$ mal ausgef\"uhrt, so ist die Wahrscheinlichkeit, dass genau $k$ dieser Versuche erfolgreich sind,
durch die Formel
\[ P(k) = \frac{n!}{k! \cdot (n-k)!} \cdot p^k \cdot (1-p)^{n-k} \]
gegeben, die auch als \emph{Binomial-Verteilung} bekannt ist.  F\"ur gro{\ss}e Werte von $n$ ist die obige Formel
sehr unhandlich, kann aber gut durch die \emph{Gau{\ss}-Verteilung} approximiert werden, es gilt
\\[0.2cm]
\hspace*{0.8cm}
$\ds\frac{n!}{k! \cdot (n-k)!} \cdot p^k \cdot (1-p)^{n-k} \approx  
   \frac{1}{\sqrt{2\cdot \pi \cdot n \cdot p \cdot (1-p)\;}} \cdot 
   \exp\left(-\frac{(k - n \cdot p)^2}{2 \cdot n \cdot p \cdot (1 - p)}\right)
$
\\[0.2cm]
Wird das Experiment $n$ mal durchgef\"uhrt, so erwarten wir im Durchschnitt nat\"urlich, dass $n \cdot p$ der
Versuche erfolgreich sein werden.  Darauf basiert unsere Sch\"atzung f\"ur den Wert von $p$, denn wir approximieren
$p$ durch die Formel
\[ p \approx \frac{k}{n}, \]
wobei $k$ die Anzahl der erfolgreichen Experimente bezeichnet.  Nun werden in der Regel nicht genau $n \cdot p$
Versuche erfolgreich sein: Zufallsbedingt werden etwas mehr oder etwas weniger Versuche
erfolgreich sein.  Das f\"uhrt dazu, dass unsere Sch\"atzung von $p$ eine Ungenauigkeit aufweist, deren ungef\"ahre
Gr\"o{\ss}e wir irgendwie absch\"atzen m\"ussen um unsere Ergebnisse beurteilen zu k\"onnen.

Um eine Idee davon zu bekommen, wie sehr die Anzahl der erfolgreichen Versuche von dem Wert $\frac{k}{n}$
abweicht,  f\"uhren wir den Begriff der \emph{Streuung} $\sigma$ ein, die f\"ur eine binomialverteilte Zufallsgr\"o{\ss}e
durch die Formel
\[ \sigma = \sqrt{n \cdot p \cdot (1 - p)} \]
gegeben ist.  Die Streuung gibt ein Ma{\ss} daf\"ur, wie stark der gemessene Wert von $k$ von dem im Mittel
erwarteten Wert $p \cdot n$ abweicht.  Es kann gezeigt werden, dass die Wahrscheinlichkeit, dass $k$ au{\ss}erhalb
des Intervalls
\[ [ p \cdot n - 3 \cdot \sigma,\, p \cdot n + 3 \cdot \sigma ] \]
liegt, also um mehr als das Dreifache von dem erwarteten Wert abweicht, kleiner als $0.27 \%$ ist.  F\"ur
 die Genauigkeit unserer Sch\"atzung $p \approx \frac{k}{n}$ hei{\ss}t das, dass dieser Sch\"atzwert mit hoher
 Wahrscheinlichkeit ($99.73\%$) in dem Intervall
\[  \left[ \frac{p \cdot n - 3 \cdot \sigma}{n},\, \frac{p \cdot n + 3 \cdot \sigma}{n} \right] 
  = \left[ p - 3 \cdot \frac{\sigma}{n},\, p + 3 \cdot \frac{\sigma}{n} \right]
\] 
liegt.  Die Genauigkeit $\varepsilon(n)$ ist durch die halbe L\"ange dieses Intervalls gegeben und hat daher den
Wert 
\[ \varepsilon(n) = 3 \cdot \frac{\sigma}{n} = 3 \cdot \sqrt{\frac{p \cdot (1 - p)}{n}}. \]
Wir erkennen hier, dass zur Erh\"ohung der Genauigkeit um den Faktor 10 die Zahl der Versuche um den Faktor 100
vergr\"o{\ss}ert werden muss.

Wenden wir die obige Formel auf die im letzen Abschnitt durchgef\"uhrte Berechnung der Zahl $\pi$ an, so erhalten
wir wegen $p = \frac{\pi}{4}$ die in Abbildung \ref{tab:Precision.java} gezeigten Ergebnisse.

\begin{table}[htbp]
  \centering
  \begin{tabular}[t]{|r|r|}
\hline
Anzahl Versuche $n$ & Genauigkeit $\varepsilon(n)$ \\
\hline
\hline
                $10$ & 0.389478 \\
\hline
               $100$ & 0.123164 \\
\hline
            $1\,000$ & 0.0389478 \\
\hline
           $10\,000$ & 0.0123164 \\
\hline
          $100\,000$ & 0.00389478 \\
\hline
       $1\,000\,000$ & 0.00123164 \\
\hline
      $10\,000\,000$ & 0.000389478 \\
\hline
     $100\,000\,000$ & 0.000123164 \\
\hline
  $1\,000\,000\,000$ & 3.89478e-05 \\
\hline
 $10\,000\,000\,000$ & 1.23164e-05 \\
\hline
$100\,000\,000\,000$ & 3.89478e-06 \\
\hline

  \end{tabular}
  \caption{Genauigkeit der Bestimung von $\pi$ bei einer Sicherheit von $99,73\%$.}
  \label{tab:Precision.java}
\end{table}

\vspace*{0.3cm}

\exercise
Wie viele Versuche sind notwendig um $\pi$ mit der Monte-Carlo-Methode auf 6
Stellen hinter dem Komma zu berechnen, wenn das Ergebnis mit einer Wahr\-scheinlichkeit von $99,73\%$
korrekt sein soll?
\vspace*{0.1cm}

\noindent
\textbf{Hinweis}: 
Um eine Genauigkeit von 6 Stellen hinter dem Komma zu erreichen, sollte der Fehler durch $10^{-7}$
abgesch\"atzt werden.  \eox
\pagebreak

\solution
Nach dem Hinweis soll 
\\[0.2cm]
\hspace*{1.3cm}
$\varepsilon(n) = 10^{-7}$
\\[0.2cm]
gelten. Setzen wir hier die Formel f\"ur $\varepsilon(n)$ ein, so erhalten wir
\\[0.2cm]
\hspace*{1.3cm}
$  
\begin{array}[t]{llcl}
                & \ds 3 \cdot \sqrt{\frac{p \cdot (1 - p)}{n}} & = & 10^{-7}   \\[0.3cm]
\Leftrightarrow & \ds 9 \cdot        \frac{p \cdot (1 - p)}{n} & = & 10^{-14}      \\[0.3cm]
\Leftrightarrow & \ds 9 \cdot    p \cdot (1 - p) \cdot 10^{14} & = & n 
\end{array}
$
\\[0.2cm] 
Um an dieser Stelle weitermachen zu k\"onnen, ben\"otigen wir den Wert der Wahr\-scheinlichkeit $p$.  Der korrekte
Wert von $p$ ist f\"ur unser Experiment durch $\frac{\pi}{4}$ gegeben.  Da wir $\pi$ ja erst berechnen wollen,
nehmen wir als N\"aherung von $\pi$ den Wert $3$, so dass $p$ den Wert $\frac{3}{4}$ hat.  Damit
ergibt sich f\"ur $n$ der Wert
\\[0.2cm]
\hspace*{1.3cm}
$n = 168.75 \cdot 10^{12}$.
\\[0.2cm]
Das sind also mehr als fast 169 Billionen Versuche. \qed


\exercise
Berechnen Sie mit Hilfe der Monte-Carlo-Methode eine N\"aherung f\"ur den Ausdruck $\mathrm{ln}(2)$.
Ihre N\"aherung soll mit einer Wahrscheinlichkeit von $99.73\%$ eine Genauigkeit von 
$\varepsilon = 10^{-3}$ haben. \eox


\section{The Monty Hall Problem}
The \href{http://en.wikipedia.org/wiki/Monty_Hall_problem}{Monty Hall problem} is famous probability
puzzle that is based on the TV show 
\href{http://en.wikipedia.org/wiki/Let%27s_Make_a_Deal}{Let's Make a Deal}, which was aired in the
US from the sixties through the seventies.  The host of this show was 
\href{http://en.wikipedia.org/wiki/Monty_Hall}{Monty Hall}. In his show, a player had to choose one
of three doors.  Monty Hall had placed goats behind two of the doors but there was a shiny new car
behind the third door.  Of course, the player did not know the location of the door with the car.  
Once the player had told Monty Hall the door he had chosen, Monty Hall would open one of the other
two doors.  However, Monty Hall would never open the door with the car behind it.  Therefore, if the
player had chosen the door with the car, Monty Hall would have randomly chosen a door leading to a
goat.  If, instead, the player had chosen a door leading to a goat, Monty Hall would have opened the
door showing the other goat.  In either case, after opening the door  Monty Hall would ask the
player whether he wanted to stick with his first choice or whether, instead, he wanted to pick the
remaining closed door.  

The question now is whether it is a good strategy to stick with the door chosen first or whether it
is better to switch doors.  Mathematically, the reasoning is quite simple: The probability that the
door chosen first leads to the car is $\frac{1}{3}$.  Therefore, the probability that the car is behind
the other unopened door has to be $\frac{2}{3}$, as the two probabilities have to add up to $1$.  

Although the reasoning given above is straightforward, many people don't believe it.  In order to
convince them, the best thing is to run a Monte Carlo simulation.  Figure \ref{fig:monty-hall.stlx}
on page 
\pageref{fig:monty-hall.stlx} shows a function that simulates \texttt{n} games and compares the
different strategies.  
\begin{enumerate}
\item The first strategy is the strategy that does not switch doors.
      For obvious reasons, this strategy is called the \emph{stupid strategy}.
\item The second strategy will always switch the the other door.
      This strategy is called the \emph{smart strategy}.
\end{enumerate}
We discuss the implementation of the function \texttt{calculateChances} line by line.
\begin{enumerate}
\item In order to compare the two strategies, the idea is to play the game offered by Monty Hall
      \texttt{n} times.  Then we need to count how many cars are won by the stupid strategy and how
      many cars are won by the smart strategy.
\item The variable \texttt{successStupid} counts the number of cars won by the stupid strategy.
\item The variable \texttt{successSmart}  counts the number of cars won by the smart  strategy.
\item The \texttt{for} loop extending from line 4 to line 15 runs \texttt{n} simulations of the
       game.
       \begin{enumerate}
       \item First, in line 5 the car is placed randomly behind one of the three doors.
       \item Second, in line 6 the player picks a door.
       \item In line 7, Monty Hall opens a door that does not have a car behind it and that
             is different from the door chosen by the player.
       \item When the player uses the smart strategy, she will then pick the remaining door in line 8.
       \end{enumerate}
\item Next, we check which of the two strategies actually wins the car.
      \begin{enumerate}
      \item If the car was placed behind the door originally chosen by the player, the stupid
            strategy wins the car.  Therefore, we increment the variable \texttt{successStupid} in
            this case.
      \item If, instead, the car was placed behind the door that was neither chosen nor opened, then
            the smart strategy wins the car.  Hence, the variable \texttt{successSmart} has to be incremented.
      \end{enumerate}
\item The function concludes by printing the results.  Running the function for \texttt{n} equal to
      $100\,000$ has yielded the following result:
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{The stupid strategy wins 33262 cars.} \\ 
      \hspace*{1.3cm}
      \texttt{The smart \ strategy wins 66738 cars.}
      \\[0.2cm]
      This shows that, on average, the payoff from the smart strategy is about twice as high as the
      payoff from the stupid strategy.  This is just what we expect since $\frac{2}{3} = 2 \cdot \frac{1}{3}$.
\end{enumerate}


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    calculateChances := procedure(n) {
        successStupid := 0;  
        successSmart  := 0; 
        for (i in [1..n]) {
            car    := rnd({1..3});                       
            choice := rnd({1..3});                      
            opened := rnd({1..3} - { choice, car });   
            last   := arb({1..3} - { choice, opened });
            if (car == choice) {
                successStupid += 1;
            }
            if (car == last) {
                successSmart += 1;
            }
        }
        print("The stupid strategy wins $successStupid$ cars.");
        print("The smart  strategy wins $successSmart $ cars.");
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{A program to solve the Monty Hall problem.}
\label{fig:monty-hall.stlx}
\end{figure}





\section[Permutations]{Erzeugung zuf\"alliger Permutationen}
In diesem Abschnitt lernen wir ein Verfahren kennen, mit dem es m\"oglich ist, eine gegebene
Liste zuf\"allig zu permutieren.  Anschaulich kann ein solches Verfahren mit dem Mischen von
Karten verglichen werden.  Das Verfahren wird auch tats\"achlich genau dazu eingesetzt:
Bei der Berechnung von Gewinn-Wahrscheinlichkeiten bei Kartenspielen wie Poker wird das
Mischen der Karten durch den gleich vorgestellten Algorithmus erledigt.  

Um eine $n$-elementige Liste $L = [x_1,x_2, \cdots, x_n]$ zuf\"allig zu permutieren,
unterscheiden wir zwei F\"alle:
\begin{enumerate}
\item Die Liste $L$ hat die L\"ange 1 und besteht folglich nur aus einem Element, $L = [x]$.
      In diesem Fall gibt die Funktion $\textsl{permute}(L)$ die Liste unver\"andert zur\"uck:
      \[ \#L = 1 \rightarrow \textsl{permute}(L) = L \]
\item Die Liste $L$ hat eine L\"ange, die gr\"o{\ss}er als $1$ ist.  In diesem Fall w\"ahlen wir
      zuf\"allig ein Element aus, das hinterher  in der zu erzeugenden Permutation an der letzten Stelle
      stehen soll.  Wir entfernen dieses Element aus der Liste und permutieren
      anschlie{\ss}end die verbleibende Liste.  An die dabei erhaltene Permutation h\"angen wir
      noch das anfangs ausgew\"ahlte Element an.  Haben wir eine Funktion
      \[ \textsl{random}: \mathbb{N} \rightarrow \mathbb{N}, \]
      so dass der Aufruf $\textsl{random}(n)$ zuf\"allig eine Zahl aus der Menge $\{1,\cdots,n\}$
      liefert, so k\"onnen wir diese \"Uberlegung wie folgt formalisieren:
      \\[0.2cm]
      \hspace*{-0.8cm}
      $\#L = n \wedge n > 1 \wedge k :=\textsl{random}(n)  \rightarrow 
         \textsl{permute}(L) = \textsl{permute}\bigl(\textsl{delete}(L,k)\bigr) + \bigl[ L(k)
         \bigr]
      $.
\\[0.2cm]
      Der  Funktionsaufruf $\textsl{delete}(L,k)$ l\"oscht dabei das $k$-te Element aus der Liste $L$,
      wir k\"onnten also schreiben
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{delete}(L,k) = L(1\,..\,k-1) + L(k+1\,..\,\#L)$.
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    permute := procedure(l) {
        if (#l == 1) {
            return l;
        }
        k := rnd([1..#l]);
        return permute(l[..k-1] + l[k+1..]) + [l[k]];
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Berechnung zuf\"alliger Permutationen eines Feldes}
\label{fig:permutation.stlx}
\end{figure}

Abbildung \ref{fig:permutation.stlx} zeigt die Umsetzung dieser Idee in \textsc{SetlX}.
Die dort gezeigte Me\-thode \textsl{permute} erzeugt eine zuf\"allige Permutation der Liste $l$, die
als Argument \"uber\-geben wird.  Die Implementierung setzt die oben beschriebenen Gleichungen
unmittelbar um.

Es kann gezeigt werden, dass der oben vorgestellte Algorithmus tats\"achlich alle Permutationen einer
gegebenen Liste mit derselben Wahrscheinlichkeit erzeugt.  Einen Beweis dieser Behauptung
finden Sie beispielsweise in \cite{cormen:01}.



%%% LOCAL Variables: 
%%% mode: latex
%%% TeX-master: "algorithms"
%%% End: 
